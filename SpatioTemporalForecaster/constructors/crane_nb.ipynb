{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import geopandas as gpd\n",
    "from pandas.tseries.offsets import DateOffset\n",
    "from shapely.geometry import Polygon\n",
    "import os\n",
    "from datetime import datetime\n",
    "from collections import namedtuple\n",
    "import ast\n",
    "import pickle\n",
    "from dataset_constructor_funcs import df_to_tensor, df_to_y_tensor, compute_adjacency_matrix\n",
    "from crane import main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "temporal_res = '2monthly'\n",
    "context_size = 5\n",
    "map_size = 'small'\n",
    "box_length_m = 500\n",
    "tsteps_to_study = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fsmuench/all-code/birds/decision-aware-topk/SpatioTemporalForecaster/constructors/crane.py:331: FutureWarning: factorize with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  gdf[DATE_NAME_TRANSLATOR[temporal_res]] = pd.factorize(list(zip(gdf['season'], gdf[DATE_NAME_TRANSLATOR[temporal_res]])))[0] + 1\n",
      "/Users/fsmuench/all-code/birds/decision-aware-topk/SpatioTemporalForecaster/constructors/crane.py:331: FutureWarning: factorize with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  gdf[DATE_NAME_TRANSLATOR[temporal_res]] = pd.factorize(list(zip(gdf['season'], gdf[DATE_NAME_TRANSLATOR[temporal_res]])))[0] + 1\n",
      "/Users/fsmuench/all-code/birds/decision-aware-topk/SpatioTemporalForecaster/constructors/crane.py:331: FutureWarning: factorize with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  gdf[DATE_NAME_TRANSLATOR[temporal_res]] = pd.factorize(list(zip(gdf['season'], gdf[DATE_NAME_TRANSLATOR[temporal_res]])))[0] + 1\n",
      "/Users/fsmuench/all-code/birds/decision-aware-topk/SpatioTemporalForecaster/constructors/crane.py:331: FutureWarning: factorize with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  gdf[DATE_NAME_TRANSLATOR[temporal_res]] = pd.factorize(list(zip(gdf['season'], gdf[DATE_NAME_TRANSLATOR[temporal_res]])))[0] + 1\n",
      "/Users/fsmuench/all-code/birds/decision-aware-topk/SpatioTemporalForecaster/constructors/crane.py:331: FutureWarning: factorize with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  gdf[DATE_NAME_TRANSLATOR[temporal_res]] = pd.factorize(list(zip(gdf['season'], gdf[DATE_NAME_TRANSLATOR[temporal_res]])))[0] + 1\n",
      "/Users/fsmuench/all-code/birds/decision-aware-topk/SpatioTemporalForecaster/constructors/crane.py:331: FutureWarning: factorize with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  gdf[DATE_NAME_TRANSLATOR[temporal_res]] = pd.factorize(list(zip(gdf['season'], gdf[DATE_NAME_TRANSLATOR[temporal_res]])))[0] + 1\n",
      "/Users/fsmuench/all-code/birds/decision-aware-topk/SpatioTemporalForecaster/constructors/crane.py:331: FutureWarning: factorize with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  gdf[DATE_NAME_TRANSLATOR[temporal_res]] = pd.factorize(list(zip(gdf['season'], gdf[DATE_NAME_TRANSLATOR[temporal_res]])))[0] + 1\n",
      "/Users/fsmuench/all-code/birds/decision-aware-topk/SpatioTemporalForecaster/constructors/crane.py:331: FutureWarning: factorize with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  gdf[DATE_NAME_TRANSLATOR[temporal_res]] = pd.factorize(list(zip(gdf['season'], gdf[DATE_NAME_TRANSLATOR[temporal_res]])))[0] + 1\n",
      "/Users/fsmuench/all-code/birds/decision-aware-topk/SpatioTemporalForecaster/constructors/crane.py:331: FutureWarning: factorize with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  gdf[DATE_NAME_TRANSLATOR[temporal_res]] = pd.factorize(list(zip(gdf['season'], gdf[DATE_NAME_TRANSLATOR[temporal_res]])))[0] + 1\n",
      "/Users/fsmuench/all-code/birds/decision-aware-topk/SpatioTemporalForecaster/constructors/crane.py:331: FutureWarning: factorize with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  gdf[DATE_NAME_TRANSLATOR[temporal_res]] = pd.factorize(list(zip(gdf['season'], gdf[DATE_NAME_TRANSLATOR[temporal_res]])))[0] + 1\n",
      "/Users/fsmuench/all-code/birds/decision-aware-topk/SpatioTemporalForecaster/constructors/crane.py:331: FutureWarning: factorize with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  gdf[DATE_NAME_TRANSLATOR[temporal_res]] = pd.factorize(list(zip(gdf['season'], gdf[DATE_NAME_TRANSLATOR[temporal_res]])))[0] + 1\n",
      "/Users/fsmuench/all-code/birds/decision-aware-topk/SpatioTemporalForecaster/constructors/crane.py:331: FutureWarning: factorize with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  gdf[DATE_NAME_TRANSLATOR[temporal_res]] = pd.factorize(list(zip(gdf['season'], gdf[DATE_NAME_TRANSLATOR[temporal_res]])))[0] + 1\n",
      "/Users/fsmuench/all-code/birds/decision-aware-topk/SpatioTemporalForecaster/constructors/crane.py:331: FutureWarning: factorize with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  gdf[DATE_NAME_TRANSLATOR[temporal_res]] = pd.factorize(list(zip(gdf['season'], gdf[DATE_NAME_TRANSLATOR[temporal_res]])))[0] + 1\n",
      "/Users/fsmuench/all-code/birds/decision-aware-topk/SpatioTemporalForecaster/constructors/crane.py:331: FutureWarning: factorize with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  gdf[DATE_NAME_TRANSLATOR[temporal_res]] = pd.factorize(list(zip(gdf['season'], gdf[DATE_NAME_TRANSLATOR[temporal_res]])))[0] + 1\n",
      "/Users/fsmuench/all-code/birds/decision-aware-topk/SpatioTemporalForecaster/constructors/crane.py:331: FutureWarning: factorize with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  gdf[DATE_NAME_TRANSLATOR[temporal_res]] = pd.factorize(list(zip(gdf['season'], gdf[DATE_NAME_TRANSLATOR[temporal_res]])))[0] + 1\n",
      "/Users/fsmuench/all-code/birds/decision-aware-topk/SpatioTemporalForecaster/constructors/crane.py:331: FutureWarning: factorize with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  gdf[DATE_NAME_TRANSLATOR[temporal_res]] = pd.factorize(list(zip(gdf['season'], gdf[DATE_NAME_TRANSLATOR[temporal_res]])))[0] + 1\n",
      "/Users/fsmuench/all-code/birds/decision-aware-topk/SpatioTemporalForecaster/constructors/crane.py:331: FutureWarning: factorize with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  gdf[DATE_NAME_TRANSLATOR[temporal_res]] = pd.factorize(list(zip(gdf['season'], gdf[DATE_NAME_TRANSLATOR[temporal_res]])))[0] + 1\n",
      "/Users/fsmuench/all-code/birds/decision-aware-topk/SpatioTemporalForecaster/constructors/crane.py:331: FutureWarning: factorize with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  gdf[DATE_NAME_TRANSLATOR[temporal_res]] = pd.factorize(list(zip(gdf['season'], gdf[DATE_NAME_TRANSLATOR[temporal_res]])))[0] + 1\n",
      "/Users/fsmuench/all-code/birds/decision-aware-topk/SpatioTemporalForecaster/constructors/crane.py:331: FutureWarning: factorize with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  gdf[DATE_NAME_TRANSLATOR[temporal_res]] = pd.factorize(list(zip(gdf['season'], gdf[DATE_NAME_TRANSLATOR[temporal_res]])))[0] + 1\n",
      "/Users/fsmuench/all-code/birds/decision-aware-topk/SpatioTemporalForecaster/constructors/crane.py:331: FutureWarning: factorize with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  gdf[DATE_NAME_TRANSLATOR[temporal_res]] = pd.factorize(list(zip(gdf['season'], gdf[DATE_NAME_TRANSLATOR[temporal_res]])))[0] + 1\n",
      "/Users/fsmuench/all-code/birds/decision-aware-topk/SpatioTemporalForecaster/constructors/crane.py:331: FutureWarning: factorize with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  gdf[DATE_NAME_TRANSLATOR[temporal_res]] = pd.factorize(list(zip(gdf['season'], gdf[DATE_NAME_TRANSLATOR[temporal_res]])))[0] + 1\n",
      "/Users/fsmuench/all-code/birds/decision-aware-topk/SpatioTemporalForecaster/constructors/crane.py:331: FutureWarning: factorize with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  gdf[DATE_NAME_TRANSLATOR[temporal_res]] = pd.factorize(list(zip(gdf['season'], gdf[DATE_NAME_TRANSLATOR[temporal_res]])))[0] + 1\n",
      "/Users/fsmuench/all-code/birds/decision-aware-topk/SpatioTemporalForecaster/constructors/crane.py:331: FutureWarning: factorize with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  gdf[DATE_NAME_TRANSLATOR[temporal_res]] = pd.factorize(list(zip(gdf['season'], gdf[DATE_NAME_TRANSLATOR[temporal_res]])))[0] + 1\n",
      "/Users/fsmuench/all-code/birds/decision-aware-topk/SpatioTemporalForecaster/constructors/crane.py:331: FutureWarning: factorize with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  gdf[DATE_NAME_TRANSLATOR[temporal_res]] = pd.factorize(list(zip(gdf['season'], gdf[DATE_NAME_TRANSLATOR[temporal_res]])))[0] + 1\n",
      "/Users/fsmuench/all-code/birds/decision-aware-topk/SpatioTemporalForecaster/constructors/crane.py:331: FutureWarning: factorize with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  gdf[DATE_NAME_TRANSLATOR[temporal_res]] = pd.factorize(list(zip(gdf['season'], gdf[DATE_NAME_TRANSLATOR[temporal_res]])))[0] + 1\n",
      "/Users/fsmuench/all-code/birds/decision-aware-topk/SpatioTemporalForecaster/constructors/crane.py:331: FutureWarning: factorize with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  gdf[DATE_NAME_TRANSLATOR[temporal_res]] = pd.factorize(list(zip(gdf['season'], gdf[DATE_NAME_TRANSLATOR[temporal_res]])))[0] + 1\n",
      "/Users/fsmuench/all-code/birds/decision-aware-topk/SpatioTemporalForecaster/constructors/crane.py:331: FutureWarning: factorize with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  gdf[DATE_NAME_TRANSLATOR[temporal_res]] = pd.factorize(list(zip(gdf['season'], gdf[DATE_NAME_TRANSLATOR[temporal_res]])))[0] + 1\n",
      "/Users/fsmuench/all-code/birds/decision-aware-topk/SpatioTemporalForecaster/constructors/crane.py:331: FutureWarning: factorize with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  gdf[DATE_NAME_TRANSLATOR[temporal_res]] = pd.factorize(list(zip(gdf['season'], gdf[DATE_NAME_TRANSLATOR[temporal_res]])))[0] + 1\n",
      "/Users/fsmuench/all-code/birds/decision-aware-topk/SpatioTemporalForecaster/constructors/crane.py:331: FutureWarning: factorize with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  gdf[DATE_NAME_TRANSLATOR[temporal_res]] = pd.factorize(list(zip(gdf['season'], gdf[DATE_NAME_TRANSLATOR[temporal_res]])))[0] + 1\n",
      "/Users/fsmuench/all-code/birds/decision-aware-topk/SpatioTemporalForecaster/constructors/crane.py:331: FutureWarning: factorize with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  gdf[DATE_NAME_TRANSLATOR[temporal_res]] = pd.factorize(list(zip(gdf['season'], gdf[DATE_NAME_TRANSLATOR[temporal_res]])))[0] + 1\n",
      "/Users/fsmuench/all-code/birds/decision-aware-topk/SpatioTemporalForecaster/constructors/crane.py:331: FutureWarning: factorize with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  gdf[DATE_NAME_TRANSLATOR[temporal_res]] = pd.factorize(list(zip(gdf['season'], gdf[DATE_NAME_TRANSLATOR[temporal_res]])))[0] + 1\n",
      "/Users/fsmuench/all-code/birds/decision-aware-topk/SpatioTemporalForecaster/constructors/crane.py:331: FutureWarning: factorize with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  gdf[DATE_NAME_TRANSLATOR[temporal_res]] = pd.factorize(list(zip(gdf['season'], gdf[DATE_NAME_TRANSLATOR[temporal_res]])))[0] + 1\n",
      "/Users/fsmuench/all-code/birds/decision-aware-topk/SpatioTemporalForecaster/constructors/crane.py:331: FutureWarning: factorize with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  gdf[DATE_NAME_TRANSLATOR[temporal_res]] = pd.factorize(list(zip(gdf['season'], gdf[DATE_NAME_TRANSLATOR[temporal_res]])))[0] + 1\n",
      "/Users/fsmuench/all-code/birds/decision-aware-topk/SpatioTemporalForecaster/constructors/crane.py:331: FutureWarning: factorize with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  gdf[DATE_NAME_TRANSLATOR[temporal_res]] = pd.factorize(list(zip(gdf['season'], gdf[DATE_NAME_TRANSLATOR[temporal_res]])))[0] + 1\n",
      "/Users/fsmuench/all-code/birds/decision-aware-topk/SpatioTemporalForecaster/constructors/crane.py:331: FutureWarning: factorize with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  gdf[DATE_NAME_TRANSLATOR[temporal_res]] = pd.factorize(list(zip(gdf['season'], gdf[DATE_NAME_TRANSLATOR[temporal_res]])))[0] + 1\n",
      "/Users/fsmuench/all-code/birds/decision-aware-topk/SpatioTemporalForecaster/constructors/crane.py:331: FutureWarning: factorize with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  gdf[DATE_NAME_TRANSLATOR[temporal_res]] = pd.factorize(list(zip(gdf['season'], gdf[DATE_NAME_TRANSLATOR[temporal_res]])))[0] + 1\n",
      "/Users/fsmuench/all-code/birds/decision-aware-topk/SpatioTemporalForecaster/constructors/crane.py:331: FutureWarning: factorize with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  gdf[DATE_NAME_TRANSLATOR[temporal_res]] = pd.factorize(list(zip(gdf['season'], gdf[DATE_NAME_TRANSLATOR[temporal_res]])))[0] + 1\n",
      "/Users/fsmuench/all-code/birds/decision-aware-topk/SpatioTemporalForecaster/constructors/crane.py:331: FutureWarning: factorize with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  gdf[DATE_NAME_TRANSLATOR[temporal_res]] = pd.factorize(list(zip(gdf['season'], gdf[DATE_NAME_TRANSLATOR[temporal_res]])))[0] + 1\n",
      "/Users/fsmuench/all-code/birds/decision-aware-topk/SpatioTemporalForecaster/constructors/crane.py:331: FutureWarning: factorize with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  gdf[DATE_NAME_TRANSLATOR[temporal_res]] = pd.factorize(list(zip(gdf['season'], gdf[DATE_NAME_TRANSLATOR[temporal_res]])))[0] + 1\n",
      "/Users/fsmuench/all-code/birds/decision-aware-topk/SpatioTemporalForecaster/constructors/crane.py:331: FutureWarning: factorize with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  gdf[DATE_NAME_TRANSLATOR[temporal_res]] = pd.factorize(list(zip(gdf['season'], gdf[DATE_NAME_TRANSLATOR[temporal_res]])))[0] + 1\n",
      "/Users/fsmuench/all-code/birds/decision-aware-topk/SpatioTemporalForecaster/constructors/crane.py:331: FutureWarning: factorize with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  gdf[DATE_NAME_TRANSLATOR[temporal_res]] = pd.factorize(list(zip(gdf['season'], gdf[DATE_NAME_TRANSLATOR[temporal_res]])))[0] + 1\n",
      "/Users/fsmuench/all-code/birds/decision-aware-topk/SpatioTemporalForecaster/constructors/crane.py:331: FutureWarning: factorize with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  gdf[DATE_NAME_TRANSLATOR[temporal_res]] = pd.factorize(list(zip(gdf['season'], gdf[DATE_NAME_TRANSLATOR[temporal_res]])))[0] + 1\n",
      "/Users/fsmuench/all-code/birds/decision-aware-topk/SpatioTemporalForecaster/constructors/crane.py:331: FutureWarning: factorize with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  gdf[DATE_NAME_TRANSLATOR[temporal_res]] = pd.factorize(list(zip(gdf['season'], gdf[DATE_NAME_TRANSLATOR[temporal_res]])))[0] + 1\n",
      "/Users/fsmuench/all-code/birds/decision-aware-topk/SpatioTemporalForecaster/constructors/crane.py:331: FutureWarning: factorize with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  gdf[DATE_NAME_TRANSLATOR[temporal_res]] = pd.factorize(list(zip(gdf['season'], gdf[DATE_NAME_TRANSLATOR[temporal_res]])))[0] + 1\n",
      "/Users/fsmuench/all-code/birds/decision-aware-topk/SpatioTemporalForecaster/constructors/crane.py:331: FutureWarning: factorize with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  gdf[DATE_NAME_TRANSLATOR[temporal_res]] = pd.factorize(list(zip(gdf['season'], gdf[DATE_NAME_TRANSLATOR[temporal_res]])))[0] + 1\n",
      "/Users/fsmuench/all-code/birds/decision-aware-topk/SpatioTemporalForecaster/constructors/crane.py:331: FutureWarning: factorize with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  gdf[DATE_NAME_TRANSLATOR[temporal_res]] = pd.factorize(list(zip(gdf['season'], gdf[DATE_NAME_TRANSLATOR[temporal_res]])))[0] + 1\n",
      "/Users/fsmuench/all-code/birds/decision-aware-topk/SpatioTemporalForecaster/constructors/crane.py:331: FutureWarning: factorize with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  gdf[DATE_NAME_TRANSLATOR[temporal_res]] = pd.factorize(list(zip(gdf['season'], gdf[DATE_NAME_TRANSLATOR[temporal_res]])))[0] + 1\n",
      "/Users/fsmuench/all-code/birds/decision-aware-topk/SpatioTemporalForecaster/constructors/crane.py:331: FutureWarning: factorize with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  gdf[DATE_NAME_TRANSLATOR[temporal_res]] = pd.factorize(list(zip(gdf['season'], gdf[DATE_NAME_TRANSLATOR[temporal_res]])))[0] + 1\n",
      "/Users/fsmuench/all-code/birds/decision-aware-topk/SpatioTemporalForecaster/constructors/crane.py:331: FutureWarning: factorize with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  gdf[DATE_NAME_TRANSLATOR[temporal_res]] = pd.factorize(list(zip(gdf['season'], gdf[DATE_NAME_TRANSLATOR[temporal_res]])))[0] + 1\n",
      "/Users/fsmuench/all-code/birds/decision-aware-topk/SpatioTemporalForecaster/constructors/crane.py:331: FutureWarning: factorize with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  gdf[DATE_NAME_TRANSLATOR[temporal_res]] = pd.factorize(list(zip(gdf['season'], gdf[DATE_NAME_TRANSLATOR[temporal_res]])))[0] + 1\n",
      "/Users/fsmuench/all-code/birds/decision-aware-topk/SpatioTemporalForecaster/constructors/crane.py:331: FutureWarning: factorize with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  gdf[DATE_NAME_TRANSLATOR[temporal_res]] = pd.factorize(list(zip(gdf['season'], gdf[DATE_NAME_TRANSLATOR[temporal_res]])))[0] + 1\n",
      "/Users/fsmuench/all-code/birds/decision-aware-topk/SpatioTemporalForecaster/constructors/crane.py:331: FutureWarning: factorize with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  gdf[DATE_NAME_TRANSLATOR[temporal_res]] = pd.factorize(list(zip(gdf['season'], gdf[DATE_NAME_TRANSLATOR[temporal_res]])))[0] + 1\n",
      "/Users/fsmuench/all-code/birds/decision-aware-topk/SpatioTemporalForecaster/constructors/crane.py:331: FutureWarning: factorize with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  gdf[DATE_NAME_TRANSLATOR[temporal_res]] = pd.factorize(list(zip(gdf['season'], gdf[DATE_NAME_TRANSLATOR[temporal_res]])))[0] + 1\n",
      "/Users/fsmuench/all-code/birds/decision-aware-topk/SpatioTemporalForecaster/constructors/crane.py:331: FutureWarning: factorize with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  gdf[DATE_NAME_TRANSLATOR[temporal_res]] = pd.factorize(list(zip(gdf['season'], gdf[DATE_NAME_TRANSLATOR[temporal_res]])))[0] + 1\n",
      "/Users/fsmuench/all-code/birds/decision-aware-topk/SpatioTemporalForecaster/constructors/crane.py:331: FutureWarning: factorize with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  gdf[DATE_NAME_TRANSLATOR[temporal_res]] = pd.factorize(list(zip(gdf['season'], gdf[DATE_NAME_TRANSLATOR[temporal_res]])))[0] + 1\n",
      "/Users/fsmuench/all-code/birds/decision-aware-topk/SpatioTemporalForecaster/constructors/crane.py:331: FutureWarning: factorize with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  gdf[DATE_NAME_TRANSLATOR[temporal_res]] = pd.factorize(list(zip(gdf['season'], gdf[DATE_NAME_TRANSLATOR[temporal_res]])))[0] + 1\n",
      "/Users/fsmuench/all-code/birds/decision-aware-topk/SpatioTemporalForecaster/constructors/crane.py:331: FutureWarning: factorize with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  gdf[DATE_NAME_TRANSLATOR[temporal_res]] = pd.factorize(list(zip(gdf['season'], gdf[DATE_NAME_TRANSLATOR[temporal_res]])))[0] + 1\n",
      "/Users/fsmuench/all-code/birds/decision-aware-topk/SpatioTemporalForecaster/constructors/crane.py:331: FutureWarning: factorize with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  gdf[DATE_NAME_TRANSLATOR[temporal_res]] = pd.factorize(list(zip(gdf['season'], gdf[DATE_NAME_TRANSLATOR[temporal_res]])))[0] + 1\n",
      "/Users/fsmuench/all-code/birds/decision-aware-topk/SpatioTemporalForecaster/constructors/crane.py:331: FutureWarning: factorize with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  gdf[DATE_NAME_TRANSLATOR[temporal_res]] = pd.factorize(list(zip(gdf['season'], gdf[DATE_NAME_TRANSLATOR[temporal_res]])))[0] + 1\n",
      "/Users/fsmuench/all-code/birds/decision-aware-topk/SpatioTemporalForecaster/constructors/crane.py:331: FutureWarning: factorize with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  gdf[DATE_NAME_TRANSLATOR[temporal_res]] = pd.factorize(list(zip(gdf['season'], gdf[DATE_NAME_TRANSLATOR[temporal_res]])))[0] + 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fsmuench/all-code/birds/decision-aware-topk/SpatioTemporalForecaster/constructors/crane.py:524: FutureWarning: factorize with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  combined_gdf['geoid'] = pd.factorize(list(zip(combined_gdf['lat'], combined_gdf['long'])))[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gdf columns Index(['geoid', 'bimonth_id', 'counts', 'lat', 'long', 'season_indicator',\n",
      "       'year'],\n",
      "      dtype='object')\n",
      "gdf index RangeIndex(start=0, stop=42816, step=1)\n"
     ]
    }
   ],
   "source": [
    "gdf, dataset_specs = main(temporal_res=temporal_res, context_size=context_size, map_size=map_size, box_length_m=box_length_m, tsteps_to_study=tsteps_to_study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geoid</th>\n",
       "      <th>bimonth_id</th>\n",
       "      <th>counts</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>season_indicator</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>148</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.004994</td>\n",
       "      <td>-97.037430</td>\n",
       "      <td>3</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>148</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.009505</td>\n",
       "      <td>-97.037348</td>\n",
       "      <td>3</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>148</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.014016</td>\n",
       "      <td>-97.037267</td>\n",
       "      <td>3</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>148</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.018527</td>\n",
       "      <td>-97.037185</td>\n",
       "      <td>3</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>148</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.023038</td>\n",
       "      <td>-97.037103</td>\n",
       "      <td>3</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42811</th>\n",
       "      <td>1333</td>\n",
       "      <td>179</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.220609</td>\n",
       "      <td>-96.687125</td>\n",
       "      <td>5</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42812</th>\n",
       "      <td>1334</td>\n",
       "      <td>179</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.220523</td>\n",
       "      <td>-96.682034</td>\n",
       "      <td>5</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42813</th>\n",
       "      <td>1335</td>\n",
       "      <td>179</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.220437</td>\n",
       "      <td>-96.676942</td>\n",
       "      <td>5</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42814</th>\n",
       "      <td>1336</td>\n",
       "      <td>179</td>\n",
       "      <td>2.0</td>\n",
       "      <td>28.224946</td>\n",
       "      <td>-96.676845</td>\n",
       "      <td>5</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42815</th>\n",
       "      <td>1337</td>\n",
       "      <td>179</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.224860</td>\n",
       "      <td>-96.671753</td>\n",
       "      <td>5</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42816 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       geoid  bimonth_id  counts        lat       long  season_indicator  year\n",
       "0          0         148     0.0  28.004994 -97.037430                 3  2000\n",
       "1          1         148     0.0  28.009505 -97.037348                 3  2000\n",
       "2          2         148     0.0  28.014016 -97.037267                 3  2000\n",
       "3          3         148     0.0  28.018527 -97.037185                 3  2000\n",
       "4          4         148     0.0  28.023038 -97.037103                 3  2000\n",
       "...      ...         ...     ...        ...        ...               ...   ...\n",
       "42811   1333         179     0.0  28.220609 -96.687125                 5  2011\n",
       "42812   1334         179     0.0  28.220523 -96.682034                 5  2011\n",
       "42813   1335         179     0.0  28.220437 -96.676942                 5  2011\n",
       "42814   1336         179     2.0  28.224946 -96.676845                 5  2011\n",
       "42815   1337         179     0.0  28.224860 -96.671753                 5  2011\n",
       "\n",
       "[42816 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lookback': 5,\n",
       " 'time_name': 'bimonth_id',\n",
       " 'space_name': 'geoid',\n",
       " 'target_name': 'counts',\n",
       " 'static': ['lat', 'long'],\n",
       " 'dynamic': [],\n",
       " 'temporal': ['season_indicator', 'year'],\n",
       " 'latlong': True}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>counts</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>season_indicator</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>geoid</th>\n",
       "      <th>bimonth_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th>148</th>\n",
       "      <td>0.0</td>\n",
       "      <td>28.004994</td>\n",
       "      <td>-97.037430</td>\n",
       "      <td>3</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>148</th>\n",
       "      <td>0.0</td>\n",
       "      <td>28.009505</td>\n",
       "      <td>-97.037348</td>\n",
       "      <td>3</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <th>148</th>\n",
       "      <td>0.0</td>\n",
       "      <td>28.014016</td>\n",
       "      <td>-97.037267</td>\n",
       "      <td>3</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <th>148</th>\n",
       "      <td>0.0</td>\n",
       "      <td>28.018527</td>\n",
       "      <td>-97.037185</td>\n",
       "      <td>3</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <th>148</th>\n",
       "      <td>0.0</td>\n",
       "      <td>28.023038</td>\n",
       "      <td>-97.037103</td>\n",
       "      <td>3</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1333</th>\n",
       "      <th>179</th>\n",
       "      <td>0.0</td>\n",
       "      <td>28.220609</td>\n",
       "      <td>-96.687125</td>\n",
       "      <td>5</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1334</th>\n",
       "      <th>179</th>\n",
       "      <td>0.0</td>\n",
       "      <td>28.220523</td>\n",
       "      <td>-96.682034</td>\n",
       "      <td>5</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1335</th>\n",
       "      <th>179</th>\n",
       "      <td>0.0</td>\n",
       "      <td>28.220437</td>\n",
       "      <td>-96.676942</td>\n",
       "      <td>5</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1336</th>\n",
       "      <th>179</th>\n",
       "      <td>2.0</td>\n",
       "      <td>28.224946</td>\n",
       "      <td>-96.676845</td>\n",
       "      <td>5</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1337</th>\n",
       "      <th>179</th>\n",
       "      <td>0.0</td>\n",
       "      <td>28.224860</td>\n",
       "      <td>-96.671753</td>\n",
       "      <td>5</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42816 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  counts        lat       long  season_indicator  year\n",
       "geoid bimonth_id                                                      \n",
       "0     148            0.0  28.004994 -97.037430                 3  2000\n",
       "1     148            0.0  28.009505 -97.037348                 3  2000\n",
       "2     148            0.0  28.014016 -97.037267                 3  2000\n",
       "3     148            0.0  28.018527 -97.037185                 3  2000\n",
       "4     148            0.0  28.023038 -97.037103                 3  2000\n",
       "...                  ...        ...        ...               ...   ...\n",
       "1333  179            0.0  28.220609 -96.687125                 5  2011\n",
       "1334  179            0.0  28.220523 -96.682034                 5  2011\n",
       "1335  179            0.0  28.220437 -96.676942                 5  2011\n",
       "1336  179            2.0  28.224946 -96.676845                 5  2011\n",
       "1337  179            0.0  28.224860 -96.671753                 5  2011\n",
       "\n",
       "[42816 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdf_new = gdf.set_index([f'geoid', 'bimonth_id'], drop=True)\n",
    "gdf_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Functions that take a T x S x F tensor and output x, y, adjacency matrix\n",
    "\"\"\"\n",
    "def df_to_tensor(df, type_='dynamic', lookback=5, time_name='bimonth_id', space_name='geoid', target_name='counts', static=None, dynamic=None, temporal=None, latlong=True, ):\n",
    "    \"\"\"\n",
    "    Converts a dataframe into a torch tensor of shape (T, S, F + lookback), where:\n",
    "      - T: number of timesteps (based on the temporal id column)\n",
    "      - S: number of spatial bins (rows for each timestep)\n",
    "      - F: number of features (columns defined by 'features' param, default ['season_indicator', 'year', 'lat', 'long'])\n",
    "      - lookback: number of lagged counts to include, from t-1 to t-lookback\n",
    "\n",
    "    For each spatial tract s at timestep t, the lagged count columns are taken from the\n",
    "    'target_name' column at the same spatial tract in previous timesteps. Missing lags are filled with NaN.\n",
    "\n",
    "    Parameters:\n",
    "      df: the input dataframe (expected to include the target_name column)\n",
    "      lookback: number of past timesteps to include as features\n",
    "      time_name: column name for temporal id\n",
    "      space_name: column name for spatial id\n",
    "      target_name: column name for counts\n",
    "      features: list of feature column names; if None, defaults to ['season_indicator', 'year', 'lat', 'long'] depending on latlong flag\n",
    "      latlong: whether to include 'lat' and 'long' in default features\n",
    "\n",
    "    Returns:\n",
    "      A torch tensor of shape (T, S, F + lookback)\n",
    "    \"\"\"    \n",
    "\n",
    "    if type_ == 'dynamic':\n",
    "\n",
    "      feats = [space_name, time_name, target_name] + dynamic\n",
    "\n",
    "    elif type_ == 'static':\n",
    "\n",
    "      feats = [space_name] + static\n",
    "    \n",
    "    elif type_ == 'temporal':\n",
    "      \n",
    "      feats = [time_name] + temporal\n",
    "\n",
    "    else: \n",
    "      raise ValueError('dataset type must be dynamic, static, or temporal')\n",
    "  \n",
    "    print('feats', feats)\n",
    "    df = df[feats]\n",
    "\n",
    "    sys.exit(0)\n",
    "\n",
    "    # Determine feature columns\n",
    "    if features is None:\n",
    "        feature_cols = [target_name]\n",
    "    else:\n",
    "        feature_cols = features\n",
    "\n",
    "    # Determine the name of the temporal id column\n",
    "    id_col = time_name\n",
    "\n",
    "    # Get sorted list of unique timesteps\n",
    "    timesteps = sorted(df[id_col].unique())\n",
    "\n",
    "    tensor_list = []\n",
    "    prev_counts_list = []  # list to hold counts tensors from previous timesteps\n",
    "\n",
    "    for t in timesteps:\n",
    "        sub_df = df[df[id_col] == t].copy().sort_values(by=space_name)\n",
    "        features_tensor = torch.tensor(sub_df[feature_cols].values, dtype=torch.float)\n",
    "\n",
    "        # Build list of lagged count tensors\n",
    "        lag_tensors = []\n",
    "        for k in range(1, lookback + 1):\n",
    "            if len(prev_counts_list) >= k:\n",
    "                lag_tensors.append(prev_counts_list[k - 1])\n",
    "            else:\n",
    "                # Backfill missing lags with NaN for the first lookback timesteps\n",
    "                lag_tensors.append(torch.full((features_tensor.shape[0], 1), float('nan'), dtype=torch.float))\n",
    "        lag_tensor = torch.cat(lag_tensors, dim=1)  # shape: (S, lookback)\n",
    "\n",
    "        # Concatenate features with lag tensors\n",
    "        combined_tensor = torch.cat([features_tensor, lag_tensor], dim=1)  # shape: (S, F + lookback)\n",
    "        tensor_list.append(combined_tensor)\n",
    "\n",
    "        # Update prev_counts_list with current counts tensor\n",
    "        current_counts = sub_df[target_name].values\n",
    "        current_counts_tensor = torch.tensor(current_counts, dtype=torch.float).unsqueeze(1)\n",
    "        prev_counts_list.insert(0, current_counts_tensor)\n",
    "        if len(prev_counts_list) > lookback:\n",
    "            prev_counts_list.pop()\n",
    "\n",
    "    # Stack along time dimension\n",
    "    result_tensor = torch.stack(tensor_list, dim=0)  # shape: (T, S, F + lookback)\n",
    "    return result_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lookback': 5,\n",
       " 'time_name': 'bimonth_id',\n",
       " 'space_name': 'geoid',\n",
       " 'target_name': 'counts',\n",
       " 'static': ['lat', 'long'],\n",
       " 'dynamic': [],\n",
       " 'temporal': ['season_indicator', 'year'],\n",
       " 'latlong': True}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geoid</th>\n",
       "      <th>bimonth_id</th>\n",
       "      <th>counts</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>season_indicator</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>148</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.004994</td>\n",
       "      <td>-97.037430</td>\n",
       "      <td>3</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>148</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.009505</td>\n",
       "      <td>-97.037348</td>\n",
       "      <td>3</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>148</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.014016</td>\n",
       "      <td>-97.037267</td>\n",
       "      <td>3</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>148</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.018527</td>\n",
       "      <td>-97.037185</td>\n",
       "      <td>3</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>148</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.023038</td>\n",
       "      <td>-97.037103</td>\n",
       "      <td>3</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42811</th>\n",
       "      <td>1333</td>\n",
       "      <td>179</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.220609</td>\n",
       "      <td>-96.687125</td>\n",
       "      <td>5</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42812</th>\n",
       "      <td>1334</td>\n",
       "      <td>179</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.220523</td>\n",
       "      <td>-96.682034</td>\n",
       "      <td>5</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42813</th>\n",
       "      <td>1335</td>\n",
       "      <td>179</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.220437</td>\n",
       "      <td>-96.676942</td>\n",
       "      <td>5</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42814</th>\n",
       "      <td>1336</td>\n",
       "      <td>179</td>\n",
       "      <td>2.0</td>\n",
       "      <td>28.224946</td>\n",
       "      <td>-96.676845</td>\n",
       "      <td>5</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42815</th>\n",
       "      <td>1337</td>\n",
       "      <td>179</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.224860</td>\n",
       "      <td>-96.671753</td>\n",
       "      <td>5</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42816 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       geoid  bimonth_id  counts        lat       long  season_indicator  year\n",
       "0          0         148     0.0  28.004994 -97.037430                 3  2000\n",
       "1          1         148     0.0  28.009505 -97.037348                 3  2000\n",
       "2          2         148     0.0  28.014016 -97.037267                 3  2000\n",
       "3          3         148     0.0  28.018527 -97.037185                 3  2000\n",
       "4          4         148     0.0  28.023038 -97.037103                 3  2000\n",
       "...      ...         ...     ...        ...        ...               ...   ...\n",
       "42811   1333         179     0.0  28.220609 -96.687125                 5  2011\n",
       "42812   1334         179     0.0  28.220523 -96.682034                 5  2011\n",
       "42813   1335         179     0.0  28.220437 -96.676942                 5  2011\n",
       "42814   1336         179     2.0  28.224946 -96.676845                 5  2011\n",
       "42815   1337         179     0.0  28.224860 -96.671753                 5  2011\n",
       "\n",
       "[42816 rows x 7 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lookback': 5,\n",
       " 'time_name': 'bimonth_id',\n",
       " 'space_name': 'geoid',\n",
       " 'target_name': 'counts',\n",
       " 'static': ['lat', 'long'],\n",
       " 'dynamic': [],\n",
       " 'temporal': ['season_indicator', 'year'],\n",
       " 'latlong': True}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import geopandas as gpd\n",
    "from pandas.tseries.offsets import DateOffset\n",
    "from shapely.geometry import Polygon\n",
    "import os\n",
    "from datetime import datetime\n",
    "from collections import namedtuple\n",
    "import ast\n",
    "import pickle\n",
    "from dataset_constructor_funcs import df_to_tensor, df_to_y_tensor, compute_adjacency_matrix\n",
    "from crane import main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lookback': 5,\n",
       " 'time_name': 'bimonth_id',\n",
       " 'space_name': 'geoid',\n",
       " 'target_name': 'counts',\n",
       " 'static': ['lat', 'long'],\n",
       " 'dynamic': [],\n",
       " 'temporal': ['season_indicator', 'year'],\n",
       " 'latlong': True}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feats ['bimonth_id', 'season_indicator', 'year']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sys' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[107], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_to_tensor(gdf, type_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemporal\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_specs)\n",
      "Cell \u001b[0;32mIn[105], line 46\u001b[0m, in \u001b[0;36mdf_to_tensor\u001b[0;34m(df, type_, lookback, time_name, space_name, target_name, static, dynamic, temporal, latlong)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeats\u001b[39m\u001b[38;5;124m'\u001b[39m, feats)\n\u001b[1;32m     44\u001b[0m df \u001b[38;5;241m=\u001b[39m df[feats]\n\u001b[0;32m---> 46\u001b[0m sys\u001b[38;5;241m.\u001b[39mexit(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Determine feature columns\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sys' is not defined"
     ]
    }
   ],
   "source": [
    "df_to_tensor(gdf, type_='temporal', **dataset_specs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARCHIVE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supported temporal scales: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Global variables\n",
    "\"\"\"\n",
    "\n",
    "# defining date range\n",
    "DATE_RANGE_TRANSLATOR = {  \n",
    "    'daily': 'D',\n",
    "    'weekly': 'W',\n",
    "    'biweekly': '2W',\n",
    "    'monthly': 'ME',\n",
    "    '2monthly': '2ME',\n",
    "}\n",
    "\n",
    "# how much temporal buffer to give based on resolution\n",
    "DATE_OFFSET_TRANSLATOR = {  \n",
    "    'daily': 1,\n",
    "    'weekly': 7,\n",
    "    'biweekly': 14,\n",
    "    'monthly': 30,\n",
    "    '2monthly': 60,\n",
    "}\n",
    "# naming the temporal column\n",
    "DATE_NAME_TRANSLATOR = {  \n",
    "    'daily': 'day',\n",
    "    'weekly': 'week',\n",
    "    'biweekly': 'biweek',\n",
    "    'monthly': 'month',\n",
    "    '2monthly': 'bimonth',\n",
    "    '3monthly': 'trimonth',\n",
    "    'seasonal': 'season'\n",
    "}\n",
    "\n",
    "MONTH_PAIRS_TRANSLATOR = {\n",
    "    '2monthly': [\"02-28\", \"04-30\", \"10-20\", \"12-25\"],\n",
    "    '3monthly': [\"01-31\", \"04-30\", \"10-20\"]\n",
    "}\n",
    "\n",
    "MAP_SIZE_TRANSLATOR = {\n",
    "    'medium': {\n",
    "        'y_left_lower_line': 0,\n",
    "        'y_right_lower_line': 0.45,\n",
    "        'y_left_upper_line': 0.35,\n",
    "        'y_right_upper_line': 0.95\n",
    "    },\n",
    "    'small': {\n",
    "        'y_left_lower_line': 0.06,\n",
    "        'y_right_lower_line': 0.72,\n",
    "        'y_left_upper_line': 0.3,\n",
    "        'y_right_upper_line': 0.5\n",
    "    }\n",
    "}\n",
    "\n",
    "SEASONAL_TRANSLATOR = {\n",
    "    9: 0,\n",
    "    10: 1,\n",
    "    11: 2,\n",
    "    12: 3,\n",
    "    1: 4,\n",
    "    2: 5,\n",
    "    3: 6, \n",
    "    4: 7\n",
    "}\n",
    "\n",
    "# meters per degree lat or long\n",
    "METERS_PER_DEGREE = 111111"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_size=5\n",
    "box_length_m=500\n",
    "map_size='small'\n",
    "years_through_2011=60\n",
    "tsteps_to_study=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# read and turn into a geopandas dataframe\n",
    "df = pd.read_csv('../../data/raw/aerial_surv/WHCR_Aerial_Observations_1950_2011.txt', encoding='latin1', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate a grid of boxes\n",
    "def generate_grid(bbox, spacing, crs):\n",
    "    \"\"\"\n",
    "    Generate box grid based on min x, min y, max x, and max y (LONG/LAT)\n",
    "    Spacing: Space between each box in degrees\n",
    "    Crs: Coordinate reference system\n",
    "    \"\"\"\n",
    "    METERS_PER_DEGREE = 111111\n",
    "\n",
    "    if crs.to_string() == 'EPSG:26914':\n",
    "        spacing = spacing * METERS_PER_DEGREE\n",
    "\n",
    "    minx, miny, maxx, maxy = bbox\n",
    "    x_coords = np.arange(minx, maxx, spacing)\n",
    "    y_coords = np.arange(miny, maxy, spacing)\n",
    "\n",
    "    grid = []\n",
    "    for x in x_coords:\n",
    "        for y in y_coords:\n",
    "            grid.append(Polygon([(x, y), (x + spacing, y), (x + spacing, y + spacing), (x, y + spacing), (x, y)]))\n",
    "    return gpd.GeoDataFrame({'geometry': grid}, crs=crs)\n",
    "\n",
    "\n",
    "def sort_date_to_range(input_date, temporal_res):\n",
    "    \"\"\"\n",
    "    Determines the range in which a given date falls based on month-day pairs.\n",
    "    \n",
    "    Parameters:\n",
    "    - input_date (datetime): The date to evaluate.\n",
    "    \n",
    "    Returns:\n",
    "    - str: The range in the format \"year-month-day_to_year-month-day\".\n",
    "    \"\"\"\n",
    "    # set possible dates for range\n",
    "    yr = input_date.year\n",
    "    curr_year = [datetime.strptime(f\"{yr}-{md}\", \"%Y-%m-%d\") for md in MONTH_PAIRS_TRANSLATOR[temporal_res]]\n",
    "    last_year = [datetime.strptime(f\"{yr - 1}-{md}\", \"%Y-%m-%d\") for md in MONTH_PAIRS_TRANSLATOR[temporal_res]]\n",
    "    next_year = [datetime.strptime(f\"{yr + 1}-{md}\", \"%Y-%m-%d\") for md in MONTH_PAIRS_TRANSLATOR[temporal_res]]\n",
    "\n",
    "    # extract correct range\n",
    "    all_dates = last_year + curr_year + next_year\n",
    "    idx = np.searchsorted(all_dates, input_date)\n",
    "    correct_range = f\"{all_dates[idx - 1].date()}_to_{all_dates[idx].date()}\"\n",
    "\n",
    "    return correct_range\n",
    "\n",
    "\n",
    "def set_date_range_2_3mo(year, temporal_res):\n",
    "\n",
    "    # Specify the year\n",
    "\n",
    "    if temporal_res == '2monthly':\n",
    "        # Create a date range for the specific dates\n",
    "        dates = [\n",
    "            pd.Timestamp(year, 10, 20),    # October 20\n",
    "            pd.Timestamp(year, 12, 24),    # December 24\n",
    "            pd.Timestamp(year + 1, 2, 27),  # Feb 28\n",
    "            pd.Timestamp(year + 1, 4, 30)      # April 30\n",
    "        ]\n",
    "    elif temporal_res == '3monthly':\n",
    "        # Create a date range for the specific dates\n",
    "        dates = [\n",
    "            pd.Timestamp(year, 10, 20),    # October 20\n",
    "            pd.Timestamp(year + 1, 1, 31),    # December 31\n",
    "            pd.Timestamp(year + 1, 4, 30)      # April 30\n",
    "        ]\n",
    "\n",
    "    # Convert to a Pandas DateTimeIndex\n",
    "    return pd.DatetimeIndex(dates)\n",
    "\n",
    "def determine_season_year(date):\n",
    "    \n",
    "    # return nan if between april 31 and oct 19\n",
    "    if (5 <= date.month <= 9) or (date.month == 10 and date.day < 20):\n",
    "        return np.nan\n",
    "\n",
    "    return date.year if date.month >= 7 else date.year - 1\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def date_range_gap(curr_drange, last_drange):\n",
    "    \"\"\"\n",
    "    Given two date-range strings of the form:\n",
    "      - \"YYYY-02-28_to_YYYY-04-30\" (spring; order 2)\n",
    "      - \"YYYY-12-25_to_YYYY-02-28\" (winter; order 1, assigned to the end year)\n",
    "      - \"YYYY-10-20_to_YYYY-12-25\" (fall; order 3)\n",
    "    returns the number of “steps” between them.\n",
    "    \"\"\"\n",
    "    def parse_range(s):\n",
    "        start_str, end_str = s.split(\"_to_\")\n",
    "        sy, sm, sd = start_str.split(\"-\")\n",
    "        ey, em, ed = end_str.split(\"-\")\n",
    "        # For winter period, we assign the order to the ending year.\n",
    "        if sm == \"12\" and sd == \"25\":\n",
    "            order = 1\n",
    "            year = int(ey)\n",
    "        elif sm == \"02\" and sd == \"28\":\n",
    "            order = 2\n",
    "            year = int(sy)  # spring: both dates in the same year\n",
    "        elif sm == \"10\" and sd == \"20\":\n",
    "            order = 3\n",
    "            year = int(sy)  # fall: both dates in the same year\n",
    "        else:\n",
    "            raise ValueError(\"Unexpected date range format: \" + s)\n",
    "        return year, order\n",
    "\n",
    "    # Parse each range\n",
    "    last_year, last_order = parse_range(last_drange)\n",
    "    curr_year, curr_order = parse_range(curr_drange)\n",
    "\n",
    "    # “Linearize” the three periods per year by assigning each an index.\n",
    "    last_index = last_year * 3 + (last_order - 1)\n",
    "    curr_index = curr_year * 3 + (curr_order - 1)\n",
    "    return curr_index - last_index\n",
    "\n",
    "def is_valid_bimonth_name(name):\n",
    "    \"\"\"\n",
    "    Returns True if the bimonth name is one of the three allowed formats:\n",
    "      - \"YYYY-02-28_to_YYYY-04-30\"\n",
    "      - \"YYYY-12-25_to_YYYY-02-28\"\n",
    "      - \"YYYY-10-20_to_YYYY-12-25\"\n",
    "    Otherwise returns False.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "\n",
    "        parts = name.split(\"_to_\")\n",
    "        if len(parts) != 2:\n",
    "            return False\n",
    "        start, end = parts\n",
    "        s_year, s_month, s_day = start.split(\"-\")\n",
    "        e_year, e_month, e_day = end.split(\"-\")\n",
    "        # Spring: 02-28 -> 04-30; same year.\n",
    "        if s_month == \"02\" and s_day == \"28\" and e_month == \"04\" and e_day == \"30\" and s_year == e_year:\n",
    "            return True\n",
    "        # Winter: 12-25 -> 02-28; note: the years differ (the winter period “belongs” to the year of Feb 28).\n",
    "        if s_month == \"12\" and s_day == \"25\" and e_month == \"02\" and e_day == \"28\":\n",
    "            return True\n",
    "        # Fall: 10-20 -> 12-25; same year.\n",
    "        if s_month == \"10\" and s_day == \"20\" and e_month == \"12\" and e_day == \"25\" and s_year == e_year:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def recalibrate_bimonth_ids(gdf, col_id='bimonth', col_name='bimonth_name'):\n",
    "    \"\"\"\n",
    "    Given a GeoDataFrame (or DataFrame) with columns:\n",
    "      - \"bimonth\" (integer ID) and\n",
    "      - \"bimonth_name\" (date range string),\n",
    "    this function does three things:\n",
    "      1. Drops any row whose bimonth_name is not valid.\n",
    "      2. Checks the gap between successive rows in both the 'bimonth' and 'bimonth_name'\n",
    "         columns.\n",
    "      3. Re-assigns (re-indexes) the bimonth IDs so that for each row, its ID equals\n",
    "         the previous row's ID plus the gap implied by the date range names.\n",
    "         (For example, if the name gap is 2 but the IDs jump by 3, the new ID will be\n",
    "          previous_ID + 2.)\n",
    "    \"\"\"\n",
    "    # Step 1: Drop rows with invalid id.\n",
    "    if col_id == 'bimonth':\n",
    "        invalid_indices = []\n",
    "        for idx, row in gdf.iterrows():\n",
    "            name = row[col_name]\n",
    "            if not is_valid_bimonth_name(name):\n",
    "                invalid_indices.append(idx)\n",
    "        if invalid_indices:\n",
    "            print(f\"Dropping rows with invalid {col_name} at indices:\", invalid_indices)\n",
    "            gdf = gdf.drop(index=invalid_indices).reset_index(drop=True)\n",
    "\n",
    "    gdf = gdf.sort_values(col_id).reset_index(drop=True)\n",
    "\n",
    "    # Step 2 & 3: Walk through the rows and reassign bimonth IDs.\n",
    "    new_ids = []\n",
    "    for i, row in gdf.iterrows():\n",
    "        if i == 0:\n",
    "            # For the first row, we can either keep the original ID or start anew.\n",
    "            new_id = row[col_id]\n",
    "            new_ids.append(new_id)\n",
    "        else:\n",
    "            prev_name = gdf.loc[i - 1, col_name]\n",
    "            curr_name = row[col_name]\n",
    "            try:\n",
    "                expected_gap = date_range_gap(curr_name, prev_name)\n",
    "            except ValueError as err:\n",
    "                # If parsing fails for some reason, skip this row.\n",
    "                print(f\"Error parsing row {i}: {err}. Dropping row.\")\n",
    "                continue\n",
    "            # Calculate new ID as previous new ID plus the expected gap.\n",
    "            new_id = new_ids[i - 1] + expected_gap\n",
    "            # (Optional) Report if the original gap did not match.\n",
    "            original_gap = row[col_id] - gdf.loc[i - 1, col_id]\n",
    "            # if original_gap != expected_gap:\n",
    "                # print(f\"Row {i}: original ID gap ({original_gap}) does not match expected ({expected_gap}). Resetting ID.\")\n",
    "            new_ids.append(new_id)\n",
    "    gdf[col_id] = new_ids\n",
    "    return gdf\n",
    "\n",
    "\n",
    "def reindex_consecutive(col):\n",
    "    # Get the unique values in sorted order.\n",
    "    sorted_unique = sorted(col.unique())\n",
    "    # Create a mapping: original value -> consecutive integer starting at 1.\n",
    "    mapping = {old_val: new_val for new_val, old_val in enumerate(sorted_unique, start=1)}\n",
    "    # Replace each value in the column with its new consecutive number.\n",
    "    return col.map(mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>long</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lat</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    long\n",
       "lat     \n",
       "1      a\n",
       "2      b\n",
       "3      c"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.DataFrame({'lat': [1, 1, 2, 2, 3, 3], 'long': ['a', 'a', 'b', 'b', 'c', 'c']})\n",
    "df_test.groupby('lat').first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_asurv(df, temporal_res):\n",
    "\n",
    "    gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.X, df.Y), crs='EPSG:26914')\n",
    "\n",
    "    # cut years based on function parameter\n",
    "    gdf = gdf[gdf['Year'].isin(gdf['Year'].unique()[-years_through_2011:])]\n",
    "\n",
    "    # add time resolution\n",
    "    gdf['date'] = pd.to_datetime(df[['Year', 'Month', 'Day']])\n",
    "\n",
    "    gdf = gdf[gdf['Month'].isin([10, 11, 12, 1, 2, 3, 4])]\n",
    "\n",
    "    if temporal_res == 'seasonal':\n",
    "\n",
    "        gdf['season'] = gdf['date'].apply(determine_season_year)\n",
    "        gdf['season_name'] = gdf['season']\n",
    "\n",
    "    elif temporal_res == '2monthly':\n",
    "\n",
    "        gdf['season'] = gdf['date'].apply(determine_season_year)\n",
    "        gdf = gdf.dropna(subset='season')\n",
    "        gdf['season'] = gdf['season'].astype('int')\n",
    "\n",
    "        all_dates = pd.DatetimeIndex([])\n",
    "        \n",
    "        for szn in gdf['season'].unique():\n",
    "\n",
    "            # set month ID\n",
    "            curr_dates = set_date_range_2_3mo(year=szn, temporal_res=temporal_res)\n",
    "            gdf.loc[gdf['season'] == szn, DATE_NAME_TRANSLATOR[temporal_res]] = np.searchsorted(curr_dates, gdf[gdf['season'] == szn]['date'])\n",
    "            gdf = gdf.sort_values(by=['season', DATE_NAME_TRANSLATOR[temporal_res]]).reset_index(drop=True)\n",
    "            gdf[DATE_NAME_TRANSLATOR[temporal_res]] = pd.factorize(list(zip(gdf['season'], gdf[DATE_NAME_TRANSLATOR[temporal_res]])))[0] + 1\n",
    "\n",
    "            all_dates = np.concatenate((all_dates, curr_dates))\n",
    "\n",
    "        gdf[f'{DATE_NAME_TRANSLATOR[temporal_res]}_name'] = gdf['date'].apply(lambda d: sort_date_to_range(d, temporal_res))\n",
    "        \n",
    "        # only keep dates that fall in the designated 2 month range. So, nothing in the summertime (april to october)\n",
    "        def extract_mo_day(date_rnge):\n",
    "            return date_rnge.split('_')[0].split('-')[1] + '-' + date_rnge.split('_')[0].split('-')[2]\n",
    "\n",
    "        valid_idxs = gdf[f'{DATE_NAME_TRANSLATOR[temporal_res]}_name'].apply(lambda rnge: extract_mo_day(rnge) != '04-30')\n",
    "        gdf = gdf[valid_idxs]\n",
    "\n",
    "        gdf = recalibrate_bimonth_ids(gdf)\n",
    "            \n",
    "    else:\n",
    "\n",
    "        all_dates = pd.date_range(start=gdf['date'].min() - DateOffset(days=DATE_OFFSET_TRANSLATOR[temporal_res]), end=gdf['date'].max() + DateOffset(days=DATE_OFFSET_TRANSLATOR[temporal_res]), freq=DATE_RANGE_TRANSLATOR[temporal_res])\n",
    "        gdf[DATE_NAME_TRANSLATOR[temporal_res]] = np.searchsorted(all_dates, gdf['date'])  \n",
    "        # add names for weeks for data clarity\n",
    "        bin_names = {i + 1: f'{all_dates[i].date()}_to_{all_dates[i + 1].date()}' for i in range(len(all_dates) - 1)}\n",
    "\n",
    "        gdf['season'] = gdf['date'].apply(determine_season_year)\n",
    "        gdf = gdf.dropna(subset=['season'])\n",
    "\n",
    "        gdf[f'{DATE_NAME_TRANSLATOR[temporal_res]}_name'] = gdf[DATE_NAME_TRANSLATOR[temporal_res]].map(bin_names)\n",
    "        gdf[DATE_NAME_TRANSLATOR[temporal_res]] = reindex_consecutive(gdf[DATE_NAME_TRANSLATOR[temporal_res]])\n",
    "        \n",
    "\n",
    "    gdf['count'] = gdf['WHITE'].fillna(0) + gdf['JUVE'].fillna(0) + gdf['UNK'].fillna(0) \n",
    "\n",
    "    complete_idx_square = True\n",
    "    keep_geometry_col = False\n",
    "    save_shp_folder = False\n",
    "\n",
    "    if keep_geometry_col:\n",
    "        columns_of_interest = ['date', f\"{DATE_NAME_TRANSLATOR[temporal_res]}\", f\"{DATE_NAME_TRANSLATOR[temporal_res]}_name\", 'X', 'Y', 'season', 'count', 'geometry']\n",
    "    else:\n",
    "        columns_of_interest = ['date', f\"{DATE_NAME_TRANSLATOR[temporal_res]}\", f\"{DATE_NAME_TRANSLATOR[temporal_res]}_name\", 'X', 'Y', 'season', 'count']\n",
    "\n",
    "    return gdf[columns_of_interest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_res='seasonal'\n",
    "gdf = read_asurv(df, temporal_res='seasonal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'crs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/m6/9kh3thbx7pv6shvwghg7mh200000gn/T/ipykernel_54069/2380819233.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mmin_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mliteral_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbounds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# MAKE SURE we are in the CRS that measures by meters, not lat/long\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfiltered_gdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'EPSG:26914'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mgrid_cells\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox_length_m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox_length_m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6295\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6296\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6297\u001b[0m         ):\n\u001b[1;32m   6298\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6299\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'crs'"
     ]
    }
   ],
   "source": [
    "all_gdfs = []\n",
    "for tstep in np.sort(gdf[f'{DATE_NAME_TRANSLATOR[temporal_res]}_name'].unique()):\n",
    "    \n",
    "    filtered_gdf = gdf[gdf[f'{DATE_NAME_TRANSLATOR[temporal_res]}_name'] == tstep]\n",
    "    # Read in bounding box from data folder\n",
    "    with open(\"../../data/raw/aerial_surv/boxes_total_bounds.txt\", \"r\") as file:\n",
    "        bounds = file.read()\n",
    "    \n",
    "    min_x, min_y, max_x, max_y = ast.literal_eval(bounds)\n",
    "\n",
    "    # MAKE SURE we are in the CRS that measures by meters, not lat/long\n",
    "    assert (filtered_gdf.crs.to_string() == 'EPSG:26914')\n",
    "    grid_cells = []\n",
    "    for x in np.arange(min_x, max_x, box_length_m):\n",
    "        for y in np.arange(min_y, max_y, box_length_m):\n",
    "            grid_cells.append(Polygon([\n",
    "                (x, y),\n",
    "                (x + box_length_m, y),\n",
    "                (x + box_length_m, y + box_length_m),\n",
    "                (x, y + box_length_m)\n",
    "            ]))\n",
    "\n",
    "    # Create a GeoDataFrame for the grid\n",
    "    full_grid = gpd.GeoDataFrame({'geometry': grid_cells}, crs=filtered_gdf.crs)\n",
    "\n",
    "    # Perform a spatial join to count the number of points in each grid cell\n",
    "    joined = gpd.sjoin(filtered_gdf, full_grid, how='left', predicate='within')\n",
    "    counts = joined.groupby('index_right').agg({'count': 'sum'})\n",
    "\n",
    "    # Add the counts to the grid GeoDataFrame\n",
    "    full_grid['counts'] = counts\n",
    "    # print(filtered_gdf.shape, filtered_gdf[f'{DATE_NAME_TRANSLATOR[temporal_res]}'].shape, type(filtered_gdf[f'{DATE_NAME_TRANSLATOR[temporal_res]}']))\n",
    "    full_grid[f'{DATE_NAME_TRANSLATOR[temporal_res]}_id'] = filtered_gdf[f'{DATE_NAME_TRANSLATOR[temporal_res]}'].unique()[0]\n",
    "    full_grid[f'{DATE_NAME_TRANSLATOR[temporal_res]}_name'] = tstep\n",
    "\n",
    "    # add unique date\n",
    "    gdf[gdf[f'{DATE_NAME_TRANSLATOR[temporal_res]}_name'] == tstep]['date'].unique()[0]\n",
    "\n",
    "    full_grid['counts'] = full_grid['counts'].fillna(0)\n",
    "\n",
    "    # add indicator\n",
    "    full_grid['season_indicator'] = SEASONAL_TRANSLATOR[datetime.strptime(tstep.split('_')[0], '%Y-%m-%d').month]\n",
    "\n",
    "    full_grid['year'] = datetime.strptime(tstep.split('_')[0], '%Y-%m-%d').year\n",
    "    \n",
    "    # print(f\"unique counts for {tstep}: {full_grid['counts'].unique()}\")\n",
    "    all_gdfs.append(full_grid)\n",
    "\n",
    "combined_gdf = pd.concat(all_gdfs)\n",
    "# print('shape of combined GDF', combined_gdf.shape[0])\n",
    "\n",
    "# create lat long columns and save to CSV\n",
    "combined_gdf.set_index([f'{DATE_NAME_TRANSLATOR[temporal_res]}_id', 'geometry'], drop=True, inplace=True)\n",
    "\n",
    "combined_gdf['geometry_col'] = combined_gdf.index.get_level_values(1)\n",
    "combined_gdf = combined_gdf.set_geometry('geometry_col')\n",
    "\n",
    "centers = combined_gdf.geometry.centroid\n",
    "centers_latlong = centers.to_crs('EPSG:4326')\n",
    "combined_gdf['lat'] = centers_latlong.y\n",
    "combined_gdf['long'] = centers_latlong.x\n",
    "\n",
    "if map_size != 'full':\n",
    "\n",
    "    # print(combined_gdf)\n",
    "    y_left_lower_line = MAP_SIZE_TRANSLATOR[map_size]['y_left_lower_line']\n",
    "    y_right_lower_line = MAP_SIZE_TRANSLATOR[map_size]['y_right_lower_line']\n",
    "    y_left_upper_line = MAP_SIZE_TRANSLATOR[map_size]['y_left_upper_line']\n",
    "    y_right_upper_line = MAP_SIZE_TRANSLATOR[map_size]['y_right_upper_line']\n",
    "\n",
    "    combined_gdf = cut_gpd_water(cut_gpd_water(combined_gdf, y1_up=y_left_lower_line, y2_up=y_right_lower_line, less=True), y1_up=y_left_upper_line, y2_up=y_right_upper_line, less=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Jacobian for all parameters at once:\n",
      "Loss: -9.999999046325684\n",
      "Jacobian shape (all together): torch.Size([10, 5, 10, 2])\n",
      "\n",
      "Computing Jacobian for each parameter separately:\n",
      "Loss: -9.999999046325684\n",
      "Jacobian shape (separate): torch.Size([10, 5, 10, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, mean=0.0, std=1.0):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.mean = nn.Parameter(torch.tensor(mean))\n",
    "        self.std = nn.Parameter(torch.tensor(std))\n",
    "\n",
    "    def sample(self, shape_TMS):\n",
    "        distribution = Normal(self.mean, torch.abs(self.std) + 1e-6)  # ensure std is positive\n",
    "        return distribution.rsample(shape_TMS)\n",
    "\n",
    "    def build_from_single_tensor(self, param_P):\n",
    "        mean, std = param_P[0], torch.abs(param_P[1]) + 1e-6\n",
    "        return Normal(mean, std)\n",
    "\n",
    "    def params_to_single_tensor(self):\n",
    "        return torch.stack([self.mean, self.std])\n",
    "\n",
    "def train_epoch(model_class, optimizer, train_T, sample_shape_MS, seed=42, compute_jacobian_together=True):\n",
    "    set_seed(seed)  # Set the random seed for reproducibility\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Initialize model\n",
    "    model = model_class()\n",
    "\n",
    "    # Sample data from the model\n",
    "    y_sample_TMS = model.sample((train_T, *sample_shape_MS))\n",
    "\n",
    "    # Calculate ratio (normalized sample values along the last dimension)\n",
    "    ratio_rating_TMS = y_sample_TMS / y_sample_TMS.sum(dim=-1, keepdim=True)\n",
    "    ratio_rating_TS = ratio_rating_TMS.mean(dim=1)\n",
    "    ratio_rating_TS.requires_grad_(True)\n",
    "\n",
    "    def get_log_probs_baked(param_P):\n",
    "        distribution = model.build_from_single_tensor(param_P)\n",
    "        return distribution.log_prob(y_sample_TMS)\n",
    "\n",
    "    if compute_jacobian_together:\n",
    "        # Compute the Jacobian of log probabilities with respect to all parameters at once\n",
    "        jac_TMSP = torch.autograd.functional.jacobian(\n",
    "            get_log_probs_baked,\n",
    "            (model.params_to_single_tensor(),),\n",
    "            strategy='forward-mode',\n",
    "            vectorize=True\n",
    "        )\n",
    "    else:\n",
    "        # Compute the Jacobian with respect to each parameter separately for efficiency and clarity\n",
    "        params = model.params_to_single_tensor()\n",
    "        jac_TMSP = []\n",
    "        for i in range(len(params)):\n",
    "            def single_param_log_prob(param_scalar):\n",
    "                param_copy = params.clone()\n",
    "                param_copy[i] = param_scalar\n",
    "                return model.build_from_single_tensor(param_copy).log_prob(y_sample_TMS)\n",
    "\n",
    "            jac = torch.autograd.functional.jacobian(\n",
    "                single_param_log_prob,\n",
    "                params[i],\n",
    "                strategy='forward-mode',\n",
    "                vectorize=True\n",
    "            )\n",
    "            jac_TMSP.append(jac)\n",
    "        jac_TMSP = torch.stack(jac_TMSP, dim=-1)\n",
    "\n",
    "    # Example loss and backward pass\n",
    "    loss = -ratio_rating_TS.sum()\n",
    "    loss.backward()\n",
    "\n",
    "    # Update model parameters if optimizer is provided\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item(), jac_TMSP\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    model_class = lambda: SimpleModel(mean=0.0, std=1.0)\n",
    "    optimizer = optim.Adam(model_class().parameters(), lr=0.01)\n",
    "\n",
    "    print(\"Computing Jacobian for all parameters at once:\")\n",
    "    loss_all, jac_all_TMSP = train_epoch(model_class, optimizer, train_T=10, sample_shape_MS=(5, 10), seed=123, compute_jacobian_together=True)\n",
    "    print(\"Loss:\", loss_all)\n",
    "    print(\"Jacobian shape (all together):\", jac_all_TMSP[0].shape)\n",
    "\n",
    "    print(\"\\nComputing Jacobian for each parameter separately:\")\n",
    "    loss_sep, jac_sep_TMSP = train_epoch(model_class, optimizer, train_T=10, sample_shape_MS=(5, 10), seed=123, compute_jacobian_together=False)\n",
    "    print(\"Loss:\", loss_sep)\n",
    "    print(\"Jacobian shape (separate):\", jac_sep_TMSP.shape)\n",
    "\n",
    "    # assert jacs are close\n",
    "    assert torch.allclose(jac_all_TMSP[0], jac_sep_TMSP, atol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 5, 10, 2])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pip_k3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
